{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From last time\n",
    "\n",
    "- we had our simple bitcask like append only database\n",
    "- it consisted of an in-memory dictionary and a append-only file\n",
    "- the append only file is an example of a write-ahead log (WAL) as well!\n",
    "\n",
    "Now for our **LSM-tree**...this is the architecture used in **LevelDB**, etc:\n",
    "\n",
    "1. when a write comes in add it first to the WAL.\n",
    "2. to an in-memory balanced tree structure, called a memtable, for example a red-black tree. The WAL above's sole purpose is to reconstruct this tree in the face of a crash\n",
    "3. At some threshold we write the tree into a SSTable (sorted string table) file. This becomes the most recent segment of the database, and the memtable is cleared out for reuse\n",
    "4. Searches go from memtable to these segmants, one by one, with a bloom filter being used to tell if the key was never there. \n",
    "5. we run merging and compaction in the background in the spirit of a mergesort and to discard overwritten or deleted values.\n",
    "\n",
    "(what about the dictionary in memory with offsets: that depends on how big a dictionary you want to keep. You dont need it because of the memtable, but it might be worth making several sparse indexes, one for each segment. while it wont tell you if a key is in that segment, it will tell you where to seek to and do a linear scan from)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Processing or Analytics?\n",
    "\n",
    "- Also known as OLTP vs OLAP/Warehousing\n",
    "- small query size vs aggregates over large ones\n",
    "- random writes from user input vs ordered ETL/stream\n",
    "- end user (amazon site) vs analyst (you)\n",
    "- GB to TB vs TB to PB\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/ETL.png)\n",
    "\n",
    "(from designing data intensive applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the different workloads\n",
    "\n",
    "- for smaller sizes any relational db will do\n",
    "- currently vendors focus on one or the other, not both\n",
    "- MS and SAP HANA support both but with different storage engines\n",
    "- OLTP need to be highly available, low latency\n",
    "- SQL (or any Pandasish syntax) is good for drilling down\n",
    "- warehousing: star schema with very wide fact table, but typically you focus on few columns at a time\n",
    "- btree indexes for oltp, bitmaps + btree for warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column oriented storage\n",
    "\n",
    "![](https://www.simple-talk.com/iwritefor/articlefiles/1844-f4cc85b0-9ddb-44cc-93ef-a742fcc4f279.jpeg)\n",
    "\n",
    "- store values from each column together in separate storage\n",
    "- lends itself to compression with bitmap indexes and run-length encoding\n",
    "- this involves choosing an appropriate sort order\n",
    "- the index then can be the data (great for IN and AND queries): there is no pointers to \"elsewhere\"\n",
    "- compressed indexes can fit into cache and are usable by iterators\n",
    "- bitwise AND/OR can be done with vector processing\n",
    "- several different sort orders can be redundantly stored\n",
    "- writing is harder: updating a row touches many column files\n",
    "- but you can write an in-memory front sorted store (row or column), and eventually merge onto the disk (Binary Tree!) (Vertica does this)\n",
    "\n",
    "![](https://www.simple-talk.com/iwritefor/articlefiles/1844-4e2482bb-aaff-4ebd-8900-1946560479af.jpeg)\n",
    "\n",
    "(images from https://www.simple-talk.com/sql/database-administration/columnstore-indexes-in-sql-server-2012/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cubes\n",
    "\n",
    "- Basically a histogram of counts in bins for multiple fields\n",
    "- can give you fast marginals and conditionals in any combination of dimensions\n",
    "- expensive to update so only used for warehousing\n",
    "- such histograms are used by query optimizers as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to indexes: B and B+-trees\n",
    "\n",
    "The log structured indexes wrote things sequentially, into segments, typically multiple MB sizes\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/btree1q.png)\n",
    "\n",
    "(from https://loveforprogramming.quora.com/Memory-locality-the-magic-of-B-Trees)\n",
    "\n",
    "- \"A linked sorted distributed range array with predefined sub array size which allows searches, sequential access, insertions, and deletions in logarithmic time. \"\n",
    "- it is a generalization of a binary tree\n",
    "- but the branching factor is much higher, and the depth thus smaller\n",
    "- brees break database into pages, and read-or-write one page at a time. A page is about 4k in size\n",
    "- leaf pages contain all the values and may represent a clustered index\n",
    "- the pointers in a btree are disk based pointers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://dl.dropboxusercontent.com/u/75194/btree1.png)\n",
    "(from designing data intensive applications)\n",
    "\n",
    "When we update a key, a split can happen\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/btree2.png)\n",
    "\n",
    "(from designing data intensive applications)\n",
    "\n",
    "This is an in-place modificaltion unlike what we had earlier. The data structure is mutable. This can cause issues for transactions, and must be dealt with. One can create immutable b-trees, and lmdb, the database inside of openldap, does this. It uses a copy-on-write schee which writes new pages elsewhere\n",
    "\n",
    "Both splits and writing in-place are dangerous, so its normal for b-tree implementations to have a WAL, or write ahead log (such a log can also be used to manage transactions). Every operation on the btree is appended to this log file.\n",
    "\n",
    "In B+ trees, pointers amonst the leaf nodes make for an easier linear scan.\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/btree2q.png)\n",
    "\n",
    "(from https://loveforprogramming.quora.com/Memory-locality-the-magic-of-B-Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B-tree vs LSM tree\n",
    "\n",
    "- critical difference: btrees update in-place!\n",
    "- comparable on random reads\n",
    "- compaction can affect performancs in LSM trees\n",
    "- LSM tree good on random writes as it makes the writes sequential\n",
    "- B-tree good for transactions; at most one place where things are: you just need to lock a section of the tree. Still, concurrency can be complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit more detail\n",
    "\n",
    "- what is a WAL based B-tree\n",
    "\n",
    "splits are dangerous. updating in place is dangerous. Thus many btree situations use a WAL. concurrency is complex as things must be changed in place\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/wal1.png)\n",
    "![](https://dl.dropboxusercontent.com/u/75194/wal2.png)\n",
    "![](https://dl.dropboxusercontent.com/u/75194/wal3.png)\n",
    "\n",
    "\n",
    "- what is Append-Only\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/append1.png)\n",
    "\n",
    "- What is copy on write\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/cow1.png)\n",
    "![](https://dl.dropboxusercontent.com/u/75194/cow2.png)\n",
    "![](https://dl.dropboxusercontent.com/u/75194/cow3.png)\n",
    "\n",
    "See http://symas.com/mdb/20141120-BuildStuff-Lightning.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an index and what is a database?\n",
    "\n",
    "- key-value indexes suppose unique keys and are thus like primary keys in a relational model\n",
    "- you can also have secondary keys or multi-column keys which can be created by some kind of concatenation or a multi-dimensional r-tree, \n",
    "- in a k-v database, the value is stored in the index and we are done. Usually in rdbms, the rows are stored in a heap file to avoid duplication, and the value of the key is **a pointer to a location in the heapfile**. This recalls our bitcask like data structure.\n",
    "- in mysql's innodb engine, the pk is a **clustered index**, while the secondary keys point to the pk.\n",
    "- a **covering index** stores certain columns in the index. Of-course in a columnar situation, the column is the index when we choose that columns ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutable BST\n",
    "\n",
    "We'll study the binary search tree as a proxy for the btree. Its not the best choice, as rebalancing and moving keys around creates havoc with seeking on disk, rather than being localizes to some pages, but its much more understandable.\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/immutablebst.png)\n",
    "\n",
    "(from purely functional data structures by Okasaki: great book BTW, with examples of balanced trees as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB: An append-only, disk based, BST\n",
    "\n",
    "Simplified and taken from http://aosabook.org/en/500L/dbdb-dog-bed-database.html, which you should read. The whole book is good, Guido's co-routineexample was from there too, and there is an example of a Transactonal Datomic-like database written in clojure there, as well.\n",
    "\n",
    "In real life this should be a balanced tree, and you could do that as a project. This might help:\n",
    "\n",
    "http://scottlobdell.me/2016/02/purely-functional-red-black-trees-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your job is to fill in the \"your code here\" parts**. \n",
    "\n",
    "Our assumption, again, is that keys and values are strings.\n",
    "\n",
    "We start by creating a \"disk reference\", henceforth knows as a \"ref\" to a object. Currently we care aboutthe actual value of a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValueRef(object):\n",
    "    \" a reference to a string value on disk\"\n",
    "    def __init__(self, referent=None, address=0):\n",
    "        self._referent = referent #value to store\n",
    "        self._address = address #address to store at\n",
    "        \n",
    "    @property\n",
    "    def address(self):\n",
    "        return self._address\n",
    "    \n",
    "    def prepare_to_store(self, storage):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def referent_to_bytes(referent):\n",
    "        return referent.encode('utf-8')\n",
    "\n",
    "    @staticmethod\n",
    "    def bytes_to_referent(bytes):\n",
    "        return bytes.decode('utf-8')\n",
    "\n",
    "    \n",
    "    def get(self, storage):\n",
    "        \"read bytes for value from disk\"\n",
    "        if self._referent is None and self._address:\n",
    "            self._referent = self.bytes_to_referent(storage.read(self._address))\n",
    "        return self._referent\n",
    "\n",
    "    def store(self, storage):\n",
    "        \"store bytes for value to disk\"\n",
    "        #called by BinaryNode.store_refs\n",
    "        if self._referent is not None and not self._address:\n",
    "            self.prepare_to_store(storage)\n",
    "            self._address = storage.write(self.referent_to_bytes(self._referent))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inherit from the `ValueRef` to create a `BinaryNodeRef` that actually stores the nodes. Note that we use pickle as out serializations cheme: this is not particularly fast but will do the job. You could implement a far tighter serialization scheme like we did last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "class BinaryNodeRef(ValueRef):\n",
    "    \"reference to a btree node on disk\"\n",
    "    \n",
    "    #calls the BinaryNode's store_refs\n",
    "    def prepare_to_store(self, storage):\n",
    "        \"have a node store its refs\"\n",
    "        if self._referent:\n",
    "            self._referent.store_refs(storage)\n",
    "\n",
    "    @staticmethod\n",
    "    def referent_to_bytes(referent):\n",
    "        \"use pickle to convert node to bytes\"\n",
    "        return pickle.dumps({\n",
    "            'left': referent.left_ref.address,\n",
    "            'key': referent.key,\n",
    "            'value': referent.value_ref.address,\n",
    "            'right': referent.right_ref.address,\n",
    "        })\n",
    "\n",
    "    @staticmethod\n",
    "    def bytes_to_referent(string):\n",
    "        \"unpickle bytes to get a node object\"\n",
    "        d = pickle.loads(string)\n",
    "        return BinaryNode(\n",
    "            BinaryNodeRef(address=d['left']),\n",
    "            d['key'],\n",
    "            ValueRef(address=d['value']),\n",
    "            BinaryNodeRef(address=d['right']),\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a node. Notice how it looks just like a binary tree node, but instead of pointers to left and right nodes, it keeps noderefs and valuerefs. This extra indirection enables us to store really large binary trees at the expense of speed. Indeed we could get the best of all worlds by memorymapping the database file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaryNode(object):\n",
    "    @classmethod\n",
    "    def from_node(cls, node, **kwargs):\n",
    "        \"clone a node with some changes from another one\"\n",
    "        return cls(\n",
    "            left_ref=kwargs.get('left_ref', node.left_ref),\n",
    "            key=kwargs.get('key', node.key),\n",
    "            value_ref=kwargs.get('value_ref', node.value_ref),\n",
    "            right_ref=kwargs.get('right_ref', node.right_ref),\n",
    "        )\n",
    "\n",
    "    def __init__(self, left_ref, key, value_ref, right_ref):\n",
    "        self.left_ref = left_ref\n",
    "        self.key = key\n",
    "        self.value_ref = value_ref\n",
    "        self.right_ref = right_ref\n",
    "\n",
    "    def store_refs(self, storage):\n",
    "        \"method for a node to store all of its stuff\"\n",
    "        self.value_ref.store(storage)\n",
    "        #calls BinaryNodeRef.store. which calls\n",
    "        #BinaryNodeRef.prepate_to_store\n",
    "        #which calls this again and recursively stores\n",
    "        #the whole tree\n",
    "        self.left_ref.store(storage)\n",
    "        self.right_ref.store(storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the tree is made of nodes with their refs to nodes and so on and so forth. You need to implement `get`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaryTree(object):\n",
    "    \"Immutable Binary Tree class. Constructs new tree on changes\"\n",
    "    def __init__(self, storage):\n",
    "        self._storage = storage\n",
    "        self._refresh_tree_ref()\n",
    "\n",
    "    def commit(self):\n",
    "        \"changes are final only when committed\"\n",
    "        #triggers BinaryNodeRef.store\n",
    "        self._tree_ref.store(self._storage)\n",
    "        #make sure address of new tree is stored\n",
    "        self._storage.commit_root_address(self._tree_ref.address)\n",
    "\n",
    "    def _refresh_tree_ref(self):\n",
    "        \"get reference to new tree if it has changed\"\n",
    "        self._tree_ref = BinaryNodeRef(\n",
    "            address=self._storage.get_root_address())\n",
    "\n",
    "    def get(self, key):\n",
    "        \"get value for a key\"\n",
    "        #if tree is not locked by another writer\n",
    "        #refresh the references and get new tree if needed\n",
    "        if not self._storage.locked:\n",
    "            self._refresh_tree_ref()\n",
    "        #your code here\n",
    "        #get the top level node\n",
    "        #traverse until you find appropriate node\n",
    "        #if key is not found raise KeyError\n",
    "\n",
    "    def set(self, key, value):\n",
    "        \"set a new value in the tree. will cause a new tree\"\n",
    "        #try to lock the tree. If we succeed make sure\n",
    "        #we dont lose updates from any other process\n",
    "        if self._storage.lock():\n",
    "            self._refresh_tree_ref()\n",
    "        #get current top-level node and make a value-ref\n",
    "        node = self._follow(self._tree_ref)\n",
    "        value_ref = ValueRef(value)\n",
    "        #insert and get new tree ref\n",
    "        self._tree_ref = self._insert(node, key, value_ref)\n",
    "        \n",
    "    \n",
    "    def _insert(self, node, key, value_ref):\n",
    "        \"insert a new node creating a new path from root\"\n",
    "        #create a tree ifnthere was none so far\n",
    "        if node is None:\n",
    "            new_node = BinaryNode(\n",
    "                BinaryNodeRef(), key, value_ref, BinaryNodeRef())\n",
    "        elif key < node.key:\n",
    "            new_node = BinaryNode.from_node(\n",
    "                node,\n",
    "                left_ref=self._insert(\n",
    "                    self._follow(node.left_ref), key, value_ref))\n",
    "        elif key > node.key:\n",
    "            new_node = BinaryNode.from_node(\n",
    "                node,\n",
    "                right_ref=self._insert(\n",
    "                    self._follow(node.right_ref), key, value_ref))\n",
    "        else: #create a new node to represent this data\n",
    "            new_node = BinaryNode.from_node(node, value_ref=value_ref)\n",
    "        return BinaryNodeRef(referent=new_node)\n",
    "\n",
    "    def delete(self, key):\n",
    "        \"delete node with key, creating new tree and path\"\n",
    "        if self._storage.lock():\n",
    "            self._refresh_tree_ref()\n",
    "        node = self._follow(self._tree_ref)\n",
    "        self._tree_ref = self._delete(node, key)\n",
    "        \n",
    "    def _delete(self, node, key):\n",
    "        \"underlying delete implementation\"\n",
    "        if node is None:\n",
    "            raise KeyError\n",
    "        elif key < node.key:\n",
    "            new_node = BinaryNode.from_node(\n",
    "                node,\n",
    "                left_ref=self._delete(\n",
    "                    self._follow(node.left_ref), key))\n",
    "        elif key > node.key:\n",
    "            new_node = BinaryNode.from_node(\n",
    "                node,\n",
    "                right_ref=self._delete(\n",
    "                    self._follow(node.right_ref), key))\n",
    "        else:\n",
    "            left = self._follow(node.left_ref)\n",
    "            right = self._follow(node.right_ref)\n",
    "            if left and right:\n",
    "                replacement = self._find_max(left)\n",
    "                left_ref = self._delete(\n",
    "                    self._follow(node.left_ref), replacement.key)\n",
    "                new_node = BinaryNode(\n",
    "                    left_ref,\n",
    "                    replacement.key,\n",
    "                    replacement.value_ref,\n",
    "                    node.right_ref,\n",
    "                )\n",
    "            elif left:\n",
    "                return node.left_ref\n",
    "            else:\n",
    "                return node.right_ref\n",
    "        return BinaryNodeRef(referent=new_node)\n",
    "\n",
    "    def _follow(self, ref):\n",
    "        \"get a node from a reference\"\n",
    "        #calls BinaryNodeRef.get\n",
    "        return ref.get(self._storage)\n",
    "    \n",
    "    def _find_max(self, node):\n",
    "        while True:\n",
    "            next_node = self._follow(node.right_ref)\n",
    "            if next_node is None:\n",
    "                return node\n",
    "            node = next_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement storage in a file. unlike in btrees where we might lock a key range, we lock the whole file on writes. In a more sophisticated software implementation of locks, we could lock subtrees instead.\n",
    "\n",
    "You need to implement `write` in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "\n",
    "import portalocker\n",
    "\n",
    "\n",
    "class Storage(object):\n",
    "    SUPERBLOCK_SIZE = 4096\n",
    "    INTEGER_FORMAT = \"!Q\"\n",
    "    INTEGER_LENGTH = 8\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self._f = f\n",
    "        self.locked = False\n",
    "        #we ensure that we start in a sector boundary\n",
    "        self._ensure_superblock()\n",
    "\n",
    "    def _ensure_superblock(self):\n",
    "        \"guarantee that the next write will start on a sector boundary\"\n",
    "        self.lock()\n",
    "        self._seek_end()\n",
    "        end_address = self._f.tell()\n",
    "        if end_address < self.SUPERBLOCK_SIZE:\n",
    "            self._f.write(b'\\x00' * (self.SUPERBLOCK_SIZE - end_address))\n",
    "        self.unlock()\n",
    "\n",
    "    def lock(self):\n",
    "        \"if not locked, lock the file for writing\"\n",
    "        if not self.locked:\n",
    "            portalocker.lock(self._f, portalocker.LOCK_EX)\n",
    "            self.locked = True\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def unlock(self):\n",
    "        if self.locked:\n",
    "            self._f.flush()\n",
    "            portalocker.unlock(self._f)\n",
    "            self.locked = False\n",
    "\n",
    "    def _seek_end(self):\n",
    "        self._f.seek(0, os.SEEK_END)\n",
    "\n",
    "    def _seek_superblock(self):\n",
    "        \"go to beginning of file which is on sec boundary\"\n",
    "        self._f.seek(0)\n",
    "\n",
    "    def _bytes_to_integer(self, integer_bytes):\n",
    "        return struct.unpack(self.INTEGER_FORMAT, integer_bytes)[0]\n",
    "\n",
    "    def _integer_to_bytes(self, integer):\n",
    "        return struct.pack(self.INTEGER_FORMAT, integer)\n",
    "\n",
    "    def _read_integer(self):\n",
    "        return self._bytes_to_integer(self._f.read(self.INTEGER_LENGTH))\n",
    "\n",
    "    def _write_integer(self, integer):\n",
    "        self.lock()\n",
    "        self._f.write(self._integer_to_bytes(integer))\n",
    "\n",
    "    def write(self, data):\n",
    "        \"write data to disk, returning the adress at which you wrote it\"\n",
    "        #first lock, get to end, get address to return, write size\n",
    "        #write data, unlock\n",
    "        #your code here\n",
    "        \n",
    "\n",
    "    def read(self, address):\n",
    "        self._f.seek(address)\n",
    "        length = self._read_integer()\n",
    "        data = self._f.read(length)\n",
    "        return data\n",
    "\n",
    "    def commit_root_address(self, root_address):\n",
    "        self.lock()\n",
    "        self._f.flush()\n",
    "        #make sure you write root address at position 0\n",
    "        self._seek_superblock()\n",
    "        #write is atomic because we store the address on a sector boundary.\n",
    "        self._write_integer(root_address)\n",
    "        self._f.flush()\n",
    "        self.unlock()\n",
    "\n",
    "    def get_root_address(self):\n",
    "        #read the first integer in the file\n",
    "        #your code here\n",
    "        self._seek_superblock()\n",
    "        root_address = self._read_integer()\n",
    "        return root_address\n",
    "\n",
    "    def close(self):\n",
    "        self.unlock()\n",
    "        self._f.close()\n",
    "\n",
    "    @property\n",
    "    def closed(self):\n",
    "        return self._f.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database is just a thin layer over the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DBDB(object):\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self._storage = Storage(f)\n",
    "        self._tree = BinaryTree(self._storage)\n",
    "\n",
    "    def _assert_not_closed(self):\n",
    "        if self._storage.closed:\n",
    "            raise ValueError('Database closed.')\n",
    "\n",
    "    def close(self):\n",
    "        self._storage.close()\n",
    "\n",
    "    def commit(self):\n",
    "        self._assert_not_closed()\n",
    "        self._tree.commit()\n",
    "\n",
    "    def get(self, key):\n",
    "        self._assert_not_closed()\n",
    "        return self._tree.get(key)\n",
    "\n",
    "    def set(self, key, value):\n",
    "        self._assert_not_closed()\n",
    "        return self._tree.set(key, value)\n",
    "\n",
    "    def delete(self, key):\n",
    "        self._assert_not_closed()\n",
    "        return self._tree.delete(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the `connect`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connect(dbname):\n",
    "    try:\n",
    "        f = open(dbname, 'r+b')\n",
    "    except IOError:\n",
    "        fd = os.open(dbname, os.O_RDWR | os.O_CREAT)\n",
    "        f = os.fdopen(fd, 'r+b')\n",
    "    return DBDB(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets play. The following ops should be reasonably clear from last time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm /tmp/test2.dbdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "db = connect(\"/tmp/test2.dbdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.set(\"rahul\", \"aged\")\n",
    "db.set(\"pavlos\", \"aged\")\n",
    "db.set(\"kobe\", \"stillyoung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-31905ff9ed7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/test2.dbdb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rahul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-d0e8d3b6ca15>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-048548199505>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_follow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "db = connect(\"/tmp/test2.dbdb\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.set(\"rahul\", \"aged\")\n",
    "db.set(\"pavlos\", \"aged\")\n",
    "db.set(\"kobe\", \"stillyoung\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aged'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.close()\n",
    "db = connect(\"/tmp/test2.dbdb\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'young'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.set(\"rahul\", \"young\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aged'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.close()\n",
    "db = connect(\"/tmp/test2.dbdb\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'young'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.set(\"rahul\", \"young\")\n",
    "db.commit()\n",
    "db.close()\n",
    "db = connect(\"/tmp/test2.dbdb\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.delete(\"pavlos\")\n",
    "db.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
