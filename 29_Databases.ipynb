{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipelines these days\n",
    "\n",
    "From @wrobstory: the SIMPLE pipelines:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/simplepipe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why learn databases\n",
    "\n",
    "- you shouldnt really implement one\n",
    "- very hard to get right, many edge cases: eg: crash, fs outta space\n",
    "- but understanding how one works is good as, data storage/munging are not just database concerns, Eg: columnar storage in Apache Parquet. And arrow, the language agnostic dataframe\n",
    "- you do need to choose a storage engine that is ok for your program, and tuning a database requires deep knowledge of the correspondence between whats the output of `explain` and whats the structure of the db\n",
    "- in particular architectures that work for transaction processing are not aptimal for analytics\n",
    "- you will use database libraries as opposed to daemons: leveldb, lmdb, bdb, sqlite\n",
    "- a lot of what you have learnt comes together here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Relational vs Document\n",
    "\n",
    "- today both are highly used: we have *polyglot persistence*\n",
    "- Mongo/Couch/etc are document oriented, store JSON documents\n",
    "- these have a higher locality of data: its like a really wide row with hierarchy\n",
    "- normalization vs denormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational Model\n",
    "\n",
    "- a relation (table) is a collection of tuples\n",
    "- SQL a declarative model: a query optimizer decides how to execute the query (if a field range covers 80% of values, should we use the index or the table?). Also parallelizable\n",
    "- *shredding* splits a document into multiple tables due to normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Model\n",
    "\n",
    "- stores nested records\n",
    "- bad for many-to-many\n",
    "- storage locality good for access, bad for writing\n",
    "- couch, mongo, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components to a database\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/dbmscomponents.png) (DBMS components from Hellerstein at al: Architecture of a Database System: circa 2007)\n",
    "\n",
    "- client connection manager: what to do with incomings\n",
    "- transactional storage\n",
    "    - storage data structures\n",
    "    - transactions and ACID: atomicity, consistency, isolation, durability\n",
    "- process model: coroutines, threads, processes\n",
    "- query model and language: query optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whats the history of a database query\n",
    "\n",
    "From Hellerstein et al:\n",
    ">At the base of the gate agent’s query plan, one or more operators exist to request data from the database. These operators make calls to fetch data from the DBMS’ Transactional Storage Manager, which man- ages all data access (read) and manipulation (create, update, delete) calls. The storage system includes algorithms and data structures for organizing and accessing data on disk (“access methods”), including basic structures like tables and indexes. It also includes a buffer management module that decides when and what data to transfer between disk and memory buffers. Returning to our example, in the course of accessing data in the access methods, the gate agent’s query must invoke the transaction management code to ensure the well-known “ACID” properties of transactions . Before accessing data, locks are acquired from a lock manager to ensure correct execution in the face of other concurrent queries.\n",
    "\n",
    ">At this point in the example query’s life, it has begun to access data records, and is ready to use them to compute results for the client. This is done by “unwinding the stack” of activities we described up to this point. The access methods return control to the query executor’s operators, which orchestrate the computation of result tuples from database data; as result tuples are generated, they are placed in a buffer for the client communications manager, which ships the results back to the caller. For large result sets, the client typically will make additional calls to fetch more data incrementally from the query, resulting in multiple itera- tions through the communications manager, query execu- tor, and storage manager. In our simple example, at the end of the query the transaction is completed and the connec- tion closed; this results in the transaction manager cleaning up state for the transaction, the process manager freeing any control structures for the query, and the communi- cations manager cleaning up communication state for the connection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Processing or Analytics?\n",
    "\n",
    "- Also known as OLTP vs OLAP/Warehousing\n",
    "- small query size vs aggregates over large ones\n",
    "- random writes from user input vs ordered ETL/stream\n",
    "- end user (amazon site) vs analyst (you)\n",
    "- GB to TB vs TB to PB\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/ETL.png)\n",
    "\n",
    "(from designing data intensive applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the different workloads\n",
    "\n",
    "- for smaller sizes any relational db will do\n",
    "- currently vendors focus on one or the other, not both\n",
    "- MS and SAP HANA support both but with different storage engines\n",
    "- OLTP need to be highly available, low latency\n",
    "- SQL (or any Pandasish syntax) is good for drilling down\n",
    "- warehousing: star schema with very wide fact table, but typically you focus on few columns at a time\n",
    "- btree indexes for oltp, bitmaps + btree for warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column oriented storage\n",
    "\n",
    "- store values from each column together in separate storage\n",
    "- lends itself to compression with bitmap indexes and run-length encoding\n",
    "- this involves choosing an appropriate sort order\n",
    "- the index then can be the data (great for IN and AND queries): there is no pinters to \"elsewhere\"\n",
    "- compressed indexes can fit into cache and are usable by iterators\n",
    "- bitwise AND/OR can be done with vector processing\n",
    "- several different sort orders can be redundantly stored\n",
    "- writing is harder: updating a row touches many column files\n",
    "- but you can write an in-memory front sorted store (row or column), and eventually merge onto the disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cubes\n",
    "\n",
    "- Basically a histogram of counts in bins for multiple fields\n",
    "- can give you fast marginals and conditionals in any combination of dimensions\n",
    "- expensive to update so only used for warehousing\n",
    "- such histograms are used by query optimizers as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Databases\n",
    "\n",
    "- an additional structure derived from the primary data\n",
    "- however a clustered index may actually store the data\n",
    "- there is overhead on writes: indexes speed up queries but slow down writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple start\n",
    "\n",
    "- start with index for key-value data\n",
    "- aka dictionary\n",
    "- in memory you are done. the index IS the database\n",
    "- hash tables are no good for range queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "database=dict()\n",
    "database['rahul']=\"aged\"\n",
    "database['pavlos']=\"ancient\"\n",
    "database['kobe']=\"stillyoung\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stillyoung'"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database['kobe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing it on disk\n",
    "\n",
    "- in the hashmap(dict) in memory, store a file offset instead\n",
    "- this file is an append only file.\n",
    "- if you update, simply append a new entry and change the offset in the hashmap\n",
    "- this is what bitcask in Riak does\n",
    "- break the file into segments. Each segment, once written, the kv pairs are never changed. maintain a hashmap per segment and search these in order\n",
    "- run compaction to throw away dupes from segments and merge them; delete old files\n",
    "- deletion is done by writing a tombstone record\n",
    "- only one writer thread. \n",
    "- bitcask will store hashmap snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://dl.dropboxusercontent.com/u/75194/riak1.png)\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/riak2.png)\n",
    "\n",
    "(from riak bitcask intro at http://basho.com/wp-content/uploads/2015/05/bitcask-intro.pdf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "class Database():\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.byteorder=sys.byteorder\n",
    "        if not os.path.exists(file):\n",
    "            self.fd = open(file, \"xb+\", buffering=0)\n",
    "            self.index={}\n",
    "        else:\n",
    "            self.fd = open(file, \"r+b\", buffering=0)\n",
    "            with open(file+\".idx\") as fdi:\n",
    "                items = [l.strip().split(':') for l in fdi.readlines()]\n",
    "                self.index = {k:int(v) for k,v in items}\n",
    "        self.readptr = self.fd.tell()\n",
    "        self.fd.seek(0,2)\n",
    "        self.writeptr = self.fd.tell()\n",
    "        \n",
    "        \n",
    "    def set(self, x, v):\n",
    "        if not isinstance(x, str):\n",
    "            raise ValueError(\"Key must be a string\")\n",
    "        bin_x = x.encode('utf-8')\n",
    "        sz_x=len(bin_x).to_bytes(1, byteorder=self.byteorder)\n",
    "        if not isinstance(v, str):\n",
    "            raise ValueError(\"Value must be a string\")\n",
    "        bin_v = v.encode('utf-8')\n",
    "        sz_v=len(bin_v).to_bytes(1, byteorder=self.byteorder)\n",
    "        try:\n",
    "            self.index[x]=self.writeptr\n",
    "            self.fd.seek(self.writeptr)\n",
    "            print(\"currently\", self.fd.tell())\n",
    "            self.fd.write(sz_x+sz_v+bin_x+bin_v)\n",
    "        except:\n",
    "            del self.index[x]\n",
    "        else:\n",
    "            self.writeptr=self.fd.tell()\n",
    "            \n",
    "    def get(self, x):\n",
    "        try:\n",
    "            offset = self.index[x]\n",
    "        except:\n",
    "            raise ValueError(\"{} is not in index\".format(x))\n",
    "        bin_x = x.encode('utf-8')\n",
    "        print(\"offset is\", offset)\n",
    "        self.readptr=offset\n",
    "        self.fd.seek(self.readptr)\n",
    "        sz_k = int.from_bytes(self.fd.read(1), byteorder=self.byteorder)\n",
    "        sz_v = int.from_bytes(self.fd.read(1), byteorder=self.byteorder)\n",
    "        self.fd.seek(sz_k,1)\n",
    "        readit=self.fd.read(sz_v).decode('utf-8')\n",
    "        print(\"now\", self.fd.tell())\n",
    "        return readit\n",
    "        \n",
    "    def close(self):\n",
    "        fdi=open(self.file+\".idx\",\"w\")\n",
    "        fdi.write(\"\\n\".join([k+\":\"+str(v) for k,v in self.index.items()]))\n",
    "        fdi.close()\n",
    "        self.fd.close()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm /tmp/test.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = Database(\"/tmp/test.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 0\n",
      "currently 11\n",
      "currently 23\n"
     ]
    }
   ],
   "source": [
    "db.set(\"rahul\", \"aged\")\n",
    "db.set(\"pavlos\", \"aged\")\n",
    "db.set(\"kobe\", \"stillyoung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pavlos': 11, 'kobe': 23, 'rahul': 0}\n"
     ]
    }
   ],
   "source": [
    "print(db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 11\n",
      "now 23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'aged'"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get(\"pavlos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 39\n"
     ]
    }
   ],
   "source": [
    "db.set(\"rahul\",\"young\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pavlos': 11, 'kobe': 23, 'rahul': 39}\n"
     ]
    }
   ],
   "source": [
    "print(db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 23\n",
      "now 39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'stillyoung'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get(\"kobe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'young'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 11\n",
      "now 23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'aged'"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get(\"pavlos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kobe': 23, 'pavlos': 11, 'rahul': 39}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 51\n"
     ]
    }
   ],
   "source": [
    "db.set(\"kobe\", \"retired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kobe': 51, 'pavlos': 11, 'rahul': 39}"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n",
      "young\n",
      "offset is 11\n",
      "now 23\n",
      "aged\n",
      "offset is 51\n",
      "now 64\n",
      "retired\n"
     ]
    }
   ],
   "source": [
    "print(db.get(\"rahul\"))\n",
    "print(db.get(\"pavlos\"))\n",
    "print(db.get(\"kobe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kobe': 51, 'obama': 64, 'pavlos': 11, 'rahul': 39}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.set(\"obama\",\"president\")\n",
    "db.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n",
      "young\n",
      "offset is 11\n",
      "now 23\n",
      "aged\n",
      "offset is 51\n",
      "now 64\n",
      "retired\n",
      "offset is 64\n",
      "now 80\n",
      "president\n"
     ]
    }
   ],
   "source": [
    "print(db.get(\"rahul\"))\n",
    "print(db.get(\"pavlos\"))\n",
    "print(db.get(\"kobe\"))\n",
    "print(db.get(\"obama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n",
      "young\n",
      "offset is 11\n",
      "now 23\n",
      "aged\n",
      "offset is 51\n",
      "now 64\n",
      "retired\n",
      "offset is 64\n",
      "now 80\n",
      "president\n"
     ]
    }
   ],
   "source": [
    "db=Database(\"/tmp/test.db\")\n",
    "print(db.get(\"rahul\"))\n",
    "print(db.get(\"pavlos\"))\n",
    "print(db.get(\"kobe\"))\n",
    "print(db.get(\"obama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kobe': 51, 'obama': 64, 'pavlos': 80, 'rahul': 39}"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.set(\"pavlos\", \"ancient\")\n",
    "db.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n",
      "young\n",
      "offset is 80\n",
      "now 95\n",
      "ancient\n",
      "offset is 51\n",
      "now 64\n",
      "retired\n",
      "offset is 64\n",
      "now 80\n",
      "president\n"
     ]
    }
   ],
   "source": [
    "print(db.get(\"rahul\"))\n",
    "print(db.get(\"pavlos\"))\n",
    "print(db.get(\"kobe\"))\n",
    "print(db.get(\"obama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pavlos:80\r\n",
      "kobe:51\r\n",
      "rahul:39\r\n",
      "obama:64"
     ]
    }
   ],
   "source": [
    "!cat /tmp/test.db.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rahulaged\r\n",
      "pavlosaged\r\n",
      "kobestillyoung\r\n",
      "rahulyoung\r\n",
      "koberetired\r\n",
      "obamapresident\r\n",
      "pavlosancient\r\n"
     ]
    }
   ],
   "source": [
    "!strings /tmp/test.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting more sophisticated\n",
    "\n",
    "### SSTables and LSM trees\n",
    "\n",
    "- keep the segments from last time\n",
    "- now add the requirement that these are sorted by key\n",
    "- merging segments is like mergesort; track recentness \n",
    "- now all keys need not be in memory, only some"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://dl.dropboxusercontent.com/u/75194/sstable.png)\n",
    "\n",
    "(from designing data intensive applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how to maintain the sort?\n",
    "- well maintain it in memory using a balanced binary tree or a memtable\n",
    "- once in-mem struct exceeds a certain size, flush to disk\n",
    "- for crashes, keep a log per memtable, to be discarded when memtable is written to a sstable. Each write is immediately appended, this is like a WAL.\n",
    "- again do lookups most recent first, and do compaction and merging. A high throughput on writes will affect compactions and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- called a LSM tree\n",
    "- popularized by bigtable\n",
    "- used in cassandra, riak, hbase, leveldb, rocksdb\n",
    "- indeed in lucent where the value is a list of documents\n",
    "- you could use it for vocabularies in your NLP.\n",
    "- leveldb uses bloom filters to prevent multiple searches if not there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most common indexing structure, btree\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/btree1q.png)\n",
    "\n",
    "(from https://loveforprogramming.quora.com/Memory-locality-the-magic-of-B-Trees)\n",
    "\n",
    "- \"A linked sorted distributed range array with predefined sub array size which allows searches, sequential access, insertions, and deletions in logarithmic time. \"\n",
    "- it is a generalization of a binary tree\n",
    "- but the branching factor is much higher, and the depth thus smaller\n",
    "- brees break database into pages, and read-or-write one page at a time\n",
    "- leaf pages contain all the values and may represent a clustered index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://dl.dropboxusercontent.com/u/75194/btree1.png)\n",
    "(from designing data intensive applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we update a key, a split can happen\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/btree2.png)\n",
    "\n",
    "(from designing data intensive applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an in-place modificaltion unlike what we had earlier. The data structure is mutable. This can cause issues for transactions, and must be dealt with. One can create immutable b-trees, and lmdb, the database inside of openldap, does this. It uses a copy-on-write schee which writes new pages elsewhere\n",
    "\n",
    "Both splits and writing in-place are dangerous, so its normal for b-tree implementations to have a WAL, or write ahead log (such a log can also be used to manage transactions). Every operation on the btree is appended to this log file.\n",
    "\n",
    "In B+ trees, pointers amonst the leaf nodes make for an easier linear scan.\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/btree2q.png)\n",
    "\n",
    "(from https://loveforprogramming.quora.com/Memory-locality-the-magic-of-B-Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immutable BST\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/immutablebst.png)\n",
    "\n",
    "(from purely functional data structures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B-tree vs LSM tree\n",
    "\n",
    "- comparable on random reads\n",
    "- LSM tree good on random writes as it makes the writes sequential\n",
    "- B-tree good for transactions; at most one place there things are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other indexes\n",
    "\n",
    "- key-value indexes suppose unique keys and are thus like primary keys in a relational model\n",
    "- you can also have secondary keys or multi-column keys which can be created by some kind of concatenation or a multi-dimensional r-tree, \n",
    "- in a k-v database, the value is stored in the index and we are done. Usually in rdbms, the rows are stored in a heap file to avoid duplication\n",
    "- in mysql's innodb engine, the pk is a clustered index, while the secondary keys point to the pk.\n",
    "- a covering index stores certain columns in the index. Of-course in a columnar situation, the column is the index when we choose that columns ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in-memory databases\n",
    "\n",
    "- are fast as byte serialization is not needed\n",
    "- in the anti-caching pattern evice LRU data to disk like virtual memory or swapping, but managed by the db. This means that you can now have an in-memory database which can handle out-of-core data.\n",
    "- h-store (now voltdb) uses this in-mamory idea with single threading to achieve high throughput in a OLTP scenario, relying on many partitions for ACID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's exercises\n",
    "\n",
    "1. Implement deletion (to submit next monday)\n",
    "2. Think about concurrency issues inour little database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 'ca'\n",
    "ba = x.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x02'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ba).to_bytes(1, byteorder='little')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "class Database():\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        # The file itself\n",
    "        self.file = file\n",
    "        \n",
    "        # Property of sys: \"little\"\n",
    "        self.byteorder=sys.byteorder\n",
    "        \n",
    "        # If the file is not in the os path, open it with \"xb+\" and initialize empty dictionary in self.index\n",
    "        if not os.path.exists(file):\n",
    "            self.fd = open(file, \"xb+\", buffering=0)\n",
    "            self.index={}\n",
    "        # If the file is in the os path, open it with \"r+b\", and strip apart everything in the file and\n",
    "        #    add it to the index, with is a dictionary of key,value pairs directly from the file\n",
    "        else:\n",
    "            self.fd = open(file, \"r+b\", buffering=0)\n",
    "            with open(file+\".idx\") as fdi:\n",
    "                items = [l.strip().split(':') for l in fdi.readlines()]\n",
    "                self.index = {k:int(v) for k,v in items}\n",
    "        # position of the read pointer is set to the current position of the pointer\n",
    "        self.readptr = self.fd.tell()\n",
    "        # Pointer is reset to the end of the file\n",
    "        self.fd.seek(0,2)\n",
    "        # The position of the write pointer is set to the end of the file.  \n",
    "        self.writeptr = self.fd.tell()\n",
    "        \n",
    "        \n",
    "    def set(self, x, v):\n",
    "        if not isinstance(x, str):\n",
    "            raise ValueError(\"Key must be a string\")\n",
    "        # Encode the input key to utf-8\n",
    "        bin_x = x.encode('utf-8')\n",
    "        # Length of the encoded key in bytes\n",
    "        sz_x=len(bin_x).to_bytes(1, byteorder=self.byteorder)\n",
    "        \n",
    "        if not isinstance(v, str):\n",
    "            raise ValueError(\"Value must be a string\")\n",
    "        # Same as above for value\n",
    "        bin_v = v.encode('utf-8')\n",
    "        sz_v=len(bin_v).to_bytes(1, byteorder=self.byteorder)\n",
    "        \n",
    "        # Try adding to the database.  Any errors, get rid of the key and reset the write pointer\n",
    "        try:\n",
    "            # Set value for the passed key to the position of the write pointer (end of file)\n",
    "            self.index[x]=self.writeptr\n",
    "            # Set the position of the pointer of the file to the end of the file\n",
    "            self.fd.seek(self.writeptr)\n",
    "            # Print the position of the pointer\n",
    "            print(\"currently\", self.fd.tell()) \n",
    "            # Write the concatenation of these four files to the end of the file\n",
    "            self.fd.write(sz_x+sz_v+bin_x+bin_v)\n",
    "        except:\n",
    "            # If there were any errors thrown above, get rid of that key from the dictionary\n",
    "            del self.index[x]\n",
    "        else: # Only hit if the try succeeds.\n",
    "            self.writeptr=self.fd.tell() # Set the write pointer to the current position (end of file)\n",
    " \n",
    "    ############ Delete added here:\n",
    "    def delete(self, x):\n",
    "        ### Check if this is a key in the dictionary\n",
    "        try:\n",
    "            self.get(x)\n",
    "        except:\n",
    "            raise ValueError(\"{} is not in database\".format(x))\n",
    "        else:\n",
    "            # Add a tombstone value at the end of the file\n",
    "            self.set(x,)\n",
    " \n",
    "        \n",
    "    def get(self, x):\n",
    "        try:\n",
    "            offset = self.index[x]\n",
    "        except:\n",
    "            raise ValueError(\"{} is not in index\".format(x))\n",
    "        bin_x = x.encode('utf-8')\n",
    "        print(\"offset is\", offset)\n",
    "        self.readptr=offset\n",
    "        self.fd.seek(self.readptr)\n",
    "        sz_k = int.from_bytes(self.fd.read(1), byteorder=self.byteorder)\n",
    "        sz_v = int.from_bytes(self.fd.read(1), byteorder=self.byteorder)\n",
    "        self.fd.seek(sz_k,1)\n",
    "        readit=self.fd.read(sz_v).decode('utf-8')\n",
    "        print(\"now\", self.fd.tell())\n",
    "        if readit ==\"9999999999\"::\n",
    "            raise KeyError(\"{} has been removed from the database\".format())\n",
    "        else:\n",
    "            return readit\n",
    "        \n",
    "    def close(self):\n",
    "        fdi=open(self.file+\".idx\",\"w\")\n",
    "        fdi.write(\"\\n\".join([k+\":\"+str(v) for k,v in self.index.items()]))\n",
    "        fdi.close()\n",
    "        self.fd.close()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.fd.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
