{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.5.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }
  }, 
  "nbformat": 4, 
  "nbformat_minor": 0, 
  "cells": [
    {
      "source": [
        "## Crawlers\n", 
        "\n", 
        "Code for this lab is almost entirely taken and modified from Brent Slatkin's Pycon 2014 talk, since it provides a beautiful illustration of the entire process."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Synchronous Blocking Crawler\n", 
        "\n", 
        "This code, taken from Brent's talk, is provided to you as an example of a synxhronous, single-threaded crawler you will make async"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 56, 
      "cell_type": "code", 
      "source": [
        "from urllib.parse import urljoin\n", 
        "from urllib.parse import urlparse\n", 
        "from urllib.parse import urlunparse\n", 
        "import re\n", 
        "import requests\n", 
        "URL_EXPR = re.compile(\n", 
        "    '([a-zA-Z]+\\s*=\\s*[\"\\'])'   # Tag attribute: href=\"\n", 
        "    '(?P<url>'\n", 
        "        '((http(s?):)?'         # Optional scheme\n", 
        "        '//[^\"\\'\\s\\\\\\\\</]+)?'   # Optional domain\n", 
        "        '/[^\"\\'\\s\\\\\\\\<]*'       # Required path\n", 
        "    ')')\n", 
        "\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 57, 
      "cell_type": "code", 
      "source": [
        "def canonicalize(url):\n", 
        "    parts = list(urlparse(url))\n", 
        "    if parts[2] == '':\n", 
        "        parts[2] = '/'  # Empty path equals root path\n", 
        "    parts[5] = ''       # Erase fragment\n", 
        "    return urlunparse(parts)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Notice the quick and dirty use of assert's here to throw exceptions if something goes wrong. The calling code should catch generic exceptions."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 58, 
      "cell_type": "code", 
      "source": [
        "def fetch(url):\n", 
        "    print(\"Doing\", url)\n", 
        "    response = requests.get(url)\n", 
        "    assert response.status_code == 200\n", 
        "    data = response.content#get as bytes\n", 
        "    assert data\n", 
        "    return data.decode('utf-8')\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 59, 
      "cell_type": "code", 
      "source": [
        "fetch(\"http://www.xkcd.com/353\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "For simplicity, we keep to the same site for now. You can pass over this code, it just extracts urls on the same domain from the page using regular expressions."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 120, 
      "cell_type": "code", 
      "source": [
        "def same_domain(a, b):\n", 
        "    parsed_a = urlparse(a)\n", 
        "    parsed_b = urlparse(b)\n", 
        "    if parsed_a.netloc == parsed_b.netloc:\n", 
        "        return True\n", 
        "    if (parsed_a.netloc == '') ^ (parsed_b.netloc == ''):  # Relative paths\n", 
        "        return True\n", 
        "    return False"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 121, 
      "cell_type": "code", 
      "source": [
        "def extract(url):\n", 
        "    data = fetch(url)\n", 
        "    found_urls = set()\n", 
        "    for match in URL_EXPR.finditer(data):\n", 
        "        found = canonicalize(match.group('url'))\n", 
        "        if same_domain(url, found):\n", 
        "            found_urls.add(urljoin(url, found))\n", 
        "    return url, len(data), sorted(found_urls)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 122, 
      "cell_type": "code", 
      "source": [
        "extract(\"http://www.xkcd.com/353\")[2]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 65, 
      "cell_type": "code", 
      "source": [
        "def extract_multi(to_fetch, seen_urls):\n", 
        "    results = []\n", 
        "    for url in to_fetch:\n", 
        "        if url in seen_urls: \n", 
        "            continue\n", 
        "        seen_urls.add(url)\n", 
        "        try:\n", 
        "            results.append(extract(url))\n", 
        "        except Exception:\n", 
        "            continue\n", 
        "    return results\n", 
        "\n", 
        "\n", 
        "def crawl(start_url, max_depth=1):\n", 
        "    seen_urls = set()\n", 
        "    to_fetch = [canonicalize(start_url)]\n", 
        "    results = []\n", 
        "    for depth in range(max_depth + 1):\n", 
        "        batch = extract_multi(to_fetch, seen_urls)\n", 
        "        to_fetch = []\n", 
        "        for url, datalen, found_urls in batch:\n", 
        "            results.append((depth, url, datalen))\n", 
        "            to_fetch.extend(found_urls)\n", 
        "\n", 
        "    return results"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 66, 
      "cell_type": "code", 
      "source": [
        "cr = crawl(\"http://www.xkcd.com/353\")\n", 
        "cr"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### 1. Synchronous crawler, async style\n", 
        "\n", 
        "(using yield from)\n", 
        "\n", 
        "Just like in the lecture, let us slowly bring in the async technology, still keeping a synchronous crawler going. This means that we'll have one `yield from` after another.\n", 
        "\n", 
        "We write the fetcher async now:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 123, 
      "cell_type": "code", 
      "source": [
        "import asyncio, aiohttp\n", 
        "\n", 
        "@asyncio.coroutine\n", 
        "def fetch_async(url):\n", 
        "    print(\"Doing\", url)\n", 
        "    response = yield from aiohttp.request('GET', url)\n", 
        "    try:\n", 
        "        assert response.status == 200\n", 
        "        data = yield from response.read()\n", 
        "        assert data\n", 
        "        return data.decode('utf-8')\n", 
        "    finally:\n", 
        "        response.close()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Write the extractor"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 124, 
      "cell_type": "code", 
      "source": [
        "@asyncio.coroutine\n", 
        "def extract_async(url):\n", 
        "    #your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We wrap the top level coroutine in a task. Since a task is a future, we can also get its result in this form."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 127, 
      "cell_type": "code", 
      "source": [
        "future = asyncio.Task(extract_async('http://www.xkcd.com/353'))\n", 
        "#future = extract_async('http://www.xkcd.com/353')\n", 
        "#you could do the above but could not access the result as \n", 
        "#future.result()\n", 
        "\n", 
        "loop = asyncio.get_event_loop()\n", 
        "loop.run_until_complete(future)\n", 
        "#loop.close() ONLY DO IF NOT IN REPL OR YOU WILL BE HOSED\n", 
        "future.result()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### 2. Write the multi-extractor and crawler\n", 
        "\n", 
        "Note that you are writing the multi-extractor using async syntax but the `yield from`s are serialized."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 94, 
      "cell_type": "code", 
      "source": [
        "@asyncio.coroutine\n", 
        "def extract_multi_async(to_fetch, seen_urls):\n", 
        "    #your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 95, 
      "cell_type": "code", 
      "source": [
        "@asyncio.coroutine\n", 
        "def crawl_async(start_url, max_depth=1):\n", 
        "    #your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We run the entire crawler now:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 96, 
      "cell_type": "code", 
      "source": [
        "future = asyncio.Task(crawl_async('http://www.xkcd.com/353', max_depth=1))\n", 
        "loop = asyncio.get_event_loop()\n", 
        "loop.run_until_complete(future)\n", 
        "future.result()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "###  3. Asynchronous crawler with `async def` and `await`: Many simultaneous fetches\n", 
        "\n", 
        "Rewrite all the code here. You will need to make two changes:\n", 
        "\n", 
        "1. `yield from` -> `await`, decorator -> `async def`\n", 
        "2. note that `extract_multi_async` upstairs was seriealized. Use futures from `asyncio.as_completed` to change this.\n", 
        "\n", 
        "The first two are just copied over"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 108, 
      "cell_type": "code", 
      "source": [
        "async def fetch_async(url):\n", 
        "    #your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 109, 
      "cell_type": "code", 
      "source": [
        "async def extract_async(url):\n", 
        "    #your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Surprisingly, one of these next two is unchanged except for the syntax. Which one? "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 110, 
      "cell_type": "code", 
      "source": [
        "async def extract_multi_async(to_fetch, seen_urls):\n", 
        "    #your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 111, 
      "cell_type": "code", 
      "source": [
        "async def crawl_async(start_url, max_depth=1):\n", 
        "    #your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 112, 
      "cell_type": "code", 
      "source": [
        "future = asyncio.Task(crawl_async('http://www.xkcd.com/353', max_depth=1))\n", 
        "loop = asyncio.get_event_loop()\n", 
        "loop.run_until_complete(future)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### 4. Concurrent Crawls\n", 
        "\n", 
        "We can even do concurrent crawls to multiple web sites. Implement this."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 113, 
      "cell_type": "code", 
      "source": [
        "urls = ['http://www.xkcd.com/353', 'http://what-if.xkcd.com/148/']"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 114, 
      "cell_type": "code", 
      "source": [
        "async def crawl_multi_async(urls):\n", 
        "    #your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 115, 
      "cell_type": "code", 
      "source": [
        "future = asyncio.Task(crawl_multi_async(urls))\n", 
        "loop = asyncio.get_event_loop()\n", 
        "loop.run_until_complete(future)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ]
}